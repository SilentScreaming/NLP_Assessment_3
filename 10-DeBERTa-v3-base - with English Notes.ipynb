{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.0.dev20250511+cu128)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'entity', 'sentiment', 'text'],\n",
      "        num_rows: 74681\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'entity', 'sentiment', 'text'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0Ô∏è‚É£ Environment & Dependencies ‚Äî Install and configure the necessary packages to ensure subsequent code runs smoothly\n",
    "# ============================================================\n",
    "\n",
    "# If transformers, datasets, torch, etc. are not installed yet, uncomment the line below to install them all at once\n",
    "!pip install transformers datasets torch scikit-learn\n",
    "!pip install peft bitsandbytes accelerate  # Required for LoRA/4-bit later\n",
    "!pip install --upgrade tiktoken\n",
    "!pip install sentencepiece\n",
    "!pip install matplotlib\n",
    "!pip install emoji\n",
    "\n",
    "import time                               # Import time module for timing\n",
    "import torch                              # Import PyTorch core library for tensor operations and autograd\n",
    "import torch.nn as nn                     # Import nn module for neural network layers (e.g., Linear, Embedding) and loss functions\n",
    "import torch.optim as optim               # Import optim module for optimizers (e.g., Adam, SGD)\n",
    "from torch.nn import functional as F      # Import functional API for common functions like ReLU, Softmax, etc.\n",
    "from torch.utils.data import DataLoader   # Import DataLoader to batch and iterate through Datasets\n",
    "\n",
    "from transformers import (                # Import core classes from HuggingFace Transformers\n",
    "    AutoTokenizer,                        # Automatically load the appropriate tokenizer for a pretrained model\n",
    "    AutoModelForSequenceClassification    # Load a pretrained model with a classification head\n",
    ")\n",
    "from datasets import load_dataset         # Import a fast-loading interface from ü§ó Datasets\n",
    "\n",
    "# Automatically select GPU (cuda) or CPU based on availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Load GitHub CSV ‚Üí DatasetDict (containing train / validation)\n",
    "# ============================================================\n",
    "\n",
    "# Import dataset directly from GitHub\n",
    "import os, io, requests\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "token = \"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\"\n",
    "\n",
    "owner  = \"SilentScreaming\"\n",
    "repo   = \"NLP_Assessment_3\"\n",
    "branch = \"main\"\n",
    "\n",
    "def fetch_csv(path_in_repo: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from a private GitHub repository and return it as a pandas DataFrame\n",
    "    \"\"\"\n",
    "    api_url = (\n",
    "        f\"https://api.github.com/repos/{owner}/{repo}/contents/\"\n",
    "        f\"{path_in_repo}?ref={branch}\"\n",
    "    )\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3.raw\",   # Key: ask the API to return raw content\n",
    "    }\n",
    "\n",
    "    resp = requests.get(api_url, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()          # Raise an error if unauthorized or path is incorrect\n",
    "\n",
    "    return pd.read_csv(\n",
    "        io.StringIO(resp.text),\n",
    "        names=[\"id\", \"entity\", \"sentiment\", \"text\"],\n",
    "        skiprows=1\n",
    "    )\n",
    "\n",
    "# Load two CSV files\n",
    "df_train = fetch_csv(\"twitter_training.csv\")\n",
    "df_val   = fetch_csv(\"twitter_validation.csv\")\n",
    "\n",
    "# Convert to Hugging Face DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\":      Dataset.from_pandas(df_train),\n",
    "    \"validation\": Dataset.from_pandas(df_val),\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂêÑÊ†áÁ≠æÊï∞ÈáèÔºö\n",
      " sentiment\n",
      "Negative      22808\n",
      "Positive      21108\n",
      "Neutral       18603\n",
      "Irrelevant    13161\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê∏ÖÊ¥óÂâç: Teh adress cant be found! LOL üòÇ Check this out: https://t.co/xyz @Friend\n",
      "Ê∏ÖÊ¥óÂêé: the address can't be found Laughing Out Loud  face_with_tears_of_joy  check this out httpstcoxyz friend\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1Ô∏è‚É£.5 Text Cleaning & Label Distribution Analysis\n",
    "# ============================================================\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# If emoji is not installed, uncomment the line below\n",
    "# !pip install emoji\n",
    "import emoji\n",
    "\n",
    "\n",
    "# ====== ‚ë† Count Four Label Types and Plot Pie Chart =====================================\n",
    "\n",
    "df_train = df_train\n",
    "df_val = df_val\n",
    "\n",
    "all_df = pd.concat([df_train, df_val], ignore_index=True)   # Merge datasets\n",
    "label_counts = all_df['sentiment'].value_counts()           # Count occurrences of each label\n",
    "print(\"Label counts:\\n\", label_counts)\n",
    "\n",
    "# Plot pie chart (default color scheme)\n",
    "plt.figure()\n",
    "plt.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Sentiment Label Distribution')\n",
    "plt.axis('equal')  # Make pie chart circular\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ====== ‚ë° Text Cleaning Functions ====================================================\n",
    "\n",
    "chat_word = {\n",
    "    # Abbreviation dictionary (chat slang ‚Üí full phrase)\n",
    "    'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard', 'ATM': 'At The Moment','A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon','BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now','BRB': 'Be Right Back','BRT': 'Be Right There','BTW': 'By The Way',\n",
    "    'B4': 'Before','CU': 'See You','CUL8R': 'See You Later','CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions','FC': 'Fingers Crossed','FWIW': \"For What It's Worth\",\n",
    "    'FYI': 'For Your Information','GAL': 'Get A Life','GG': 'Good Game','GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike','GR8': 'Great!','G9': 'Genius','IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)','ILU': 'I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion','IMO': 'In My Opinion','IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life','KISS': 'Keep It Simple, Stupid','LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off','LOL': 'Laughing Out Loud','LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later','MTE': 'My Thoughts Exactly','M8': 'Mate','NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See','PITA': 'Pain In The A..','PRT': 'Party','PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?','ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud','ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate','STATS': 'Your sex and age','ASL': 'Age, Sex, Location','THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!','TTYL': 'Talk To You Later','U': 'You','U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever','WB': 'Welcome Back','WTF': 'What The F...','WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?','W8': 'Wait...','7K': 'Sick:-D Laugher','TFW': 'That feeling when',\n",
    "    'MFW': 'My face when','MRW': 'My reaction when','IFYP': 'I feel your pain',\n",
    "    'TNTL': 'Trying not to laugh','JK': 'Just kidding','IDC': \"I don't care\",'ILY': 'I love you',\n",
    "    'IMU': 'I miss you','ADIH': 'Another day in hell','ZZZ': 'Sleeping, bored, tired',\n",
    "    'WYWH': 'Wish you were here','TIME': 'Tears in my eyes','BAE': 'Before anyone else',\n",
    "    'FIMH': 'Forever in my heart','BSAAW': 'Big smile and a wink','BWL': 'Bursting with laughter',\n",
    "    'BFF': 'Best friends forever','CSL': \"Can't stop laughing\"\n",
    "}\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "\n",
    "def remove_mentions(text: str) -> str:\n",
    "    \"\"\"Remove @username mentions\"\"\"\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "\n",
    "def emoji_to_text(text: str) -> str:\n",
    "    \"\"\"Convert emojis to their corresponding text descriptions\"\"\"\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "\n",
    "def expand_chat_abbrev(text: str, mapping: dict = chat_word) -> str:\n",
    "    \"\"\"Expand abbreviations using the chat_word dictionary\"\"\"\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, mapping.keys())) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    def replace(match):\n",
    "        word = match.group(0)\n",
    "        return mapping.get(word.upper(), word)\n",
    "\n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "\n",
    "def to_lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize casing ‚Äî recommended as the first step.\n",
    "    Converts all characters to lowercase so 'Hello' and 'hello' are treated equally.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Remove English punctuation\n",
    "_punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove English punctuation using str.translate.\n",
    "    (To remove Chinese punctuation as well, you can extend _punct_table)\n",
    "    \"\"\"\n",
    "    return text.translate(_punct_table)\n",
    "\n",
    "\n",
    "# Correct common spelling mistakes\n",
    "common_misspellings = {\n",
    "    'teh': 'the',\n",
    "    'recieve': 'receive',\n",
    "    'definately': 'definitely',\n",
    "    'seperate': 'separate',\n",
    "    'occured': 'occurred',\n",
    "    'thier': 'their',\n",
    "    'adress': 'address',\n",
    "    'wich': 'which',\n",
    "    'becuase': 'because',\n",
    "    'goverment': 'government',\n",
    "    'enviroment': 'environment',\n",
    "    'immediatly': 'immediately',\n",
    "    'publically': 'publicly',\n",
    "    'succesful': 'successful',\n",
    "    'untill': 'until',\n",
    "    'arguement': 'argument',\n",
    "    'wierd': 'weird',\n",
    "    'alot': 'a lot',\n",
    "    'cant': \"can't\",\n",
    "    'wont': \"won't\",\n",
    "}\n",
    "\n",
    "def correct_misspellings(text: str, mapping: dict = common_misspellings) -> str:\n",
    "    \"\"\"\n",
    "    Correct common spelling errors using the provided mapping dictionary.\n",
    "    Assumes input text is already in lowercase.\n",
    "    \"\"\"\n",
    "    if not mapping:\n",
    "        return text\n",
    "\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, mapping.keys())) + r')\\b')\n",
    "    return pattern.sub(lambda m: mapping[m.group(0)], text)\n",
    "\n",
    "\n",
    "# Example test\n",
    "sample = \"Teh adress cant be found! LOL üòÇ Check this out: https://t.co/xyz @Friend\"\n",
    "\n",
    "cleaned = expand_chat_abbrev(             # 4. Expand abbreviations\n",
    "             correct_misspellings(        # 3. Correct spelling\n",
    "                 emoji_to_text(           # 2. Convert emojis to text\n",
    "                     remove_mentions(     # 1. Remove @mentions\n",
    "                         remove_urls(     # 0. Remove URLs\n",
    "                             remove_punctuation(   # Remove punctuation\n",
    "                                 to_lower(sample)  # -1. Convert to lowercase\n",
    "                             )\n",
    "                         )\n",
    "                     )\n",
    "                 )\n",
    "             )\n",
    "           )\n",
    "\n",
    "print(\"Before cleaning:\", sample)\n",
    "print(\"After cleaning:\", cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3284006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74681/74681 [00:33<00:00, 2234.52 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:00<00:00, 2522.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'entity', 'sentiment', 'text'],\n",
      "        num_rows: 74681\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'entity', 'sentiment', 'text'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0Ô∏è‚É£ Ensure all 7 basic preprocessing functions are loaded into memory\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Text Cleaning Pipeline: For Individual Samples\n",
    "# ------------------------------------------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform preprocessing on a single tweet/text string:\n",
    "      1. Convert to lowercase\n",
    "      2. Remove English punctuation\n",
    "      3. Remove URLs\n",
    "      4. Remove @mentions\n",
    "      5. Convert emojis to text\n",
    "      6. Correct common spelling errors\n",
    "      7. Expand chat abbreviations\n",
    "    Returns the cleaned string.\n",
    "    \"\"\"\n",
    "    if text is None or not isinstance(text, str):\n",
    "        return \"\"  # Safety check: return empty string for None or invalid input\n",
    "\n",
    "    return expand_chat_abbrev(            # 7. Expand abbreviations\n",
    "             correct_misspellings(        # 6. Correct spelling errors\n",
    "               emoji_to_text(             # 5. Emoji to text\n",
    "                 remove_mentions(         # 4. Remove mentions\n",
    "                   remove_urls(           # 3. Remove URLs\n",
    "                     remove_punctuation(  # 2. Remove punctuation\n",
    "                       to_lower(text)     # 1. Convert to lowercase\n",
    "                     )\n",
    "                   )\n",
    "                 )\n",
    "               )\n",
    "             )\n",
    "           )\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ Batch Cleaning Function for use with Dataset.map\n",
    "# ------------------------------------------------------------\n",
    "def clean_batch(batch):\n",
    "    \"\"\"\n",
    "    Callback for Dataset.map(batched=True):\n",
    "    Receives a batch (dict), applies cleaning to the 'text' field,\n",
    "    and writes the result back to the same column or to a new 'clean_text' column.\n",
    "    \"\"\"\n",
    "    # Assumes the column is named 'text'; modify this if your column name is different\n",
    "    batch['text'] = [clean_text(t) for t in batch['text']]\n",
    "    # If you prefer to keep the original text and write results to 'clean_text', use:\n",
    "    # batch['clean_text'] = [clean_text(t) for t in batch['text']]\n",
    "    return batch\n",
    "\n",
    "## Note:\n",
    "# Wrap tokenizer + preprocessing + DataLoader into a reusable config object\n",
    "# ----------------- Configuration (can be managed via argparse / OmegaConf) -----------------\n",
    "from types import SimpleNamespace\n",
    "cfg = SimpleNamespace(\n",
    "    teacher_name = \"microsoft/deberta-v3-base\",\n",
    "    student_type = \"bert-tiny\",\n",
    "    use_lora     = False,\n",
    "    lora_cfg     = dict(r=16, alpha=32, dropout=0.05),\n",
    "    device       = \"cuda\",\n",
    "    batch_size   = 32,\n",
    "    max_length   = 128,\n",
    "    num_proc     = 4,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Apply to DatasetDict\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "dataset = dataset.map(\n",
    "    clean_batch,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    "    # num_proc=min(os.cpu_count(), cfg.num_proc)  # Can increase based on CPU cores for faster mapping (do not change unless you understand the potential issues)\n",
    ")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e24d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:03<00:00, 5415.95 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:00<00:00, 11378.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.30.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:05<00:00, 3841.88 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:00<00:00, 3815.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2Ô∏è‚É£ Text Labels ‚Üí Numeric Labels\n",
    "# ============================================================\n",
    "\n",
    "# Define the mapping from sentiment string labels to integer values (keeping all four classes)\n",
    "label2id = {'Negative': 0, 'Neutral': 1, 'Positive': 2, 'Irrelevant': 3}\n",
    "# Reverse mapping, used to convert predicted label IDs back to strings during saving or inference\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def add_numeric_label(example):\n",
    "    \"\"\"\n",
    "    Convert the sentiment string label to its corresponding numeric value\n",
    "    and add it to the sample dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    example : Dict\n",
    "        A single data example containing at least the key 'sentiment'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        The updated dictionary with an added key 'label' containing the integer label\n",
    "    \"\"\"\n",
    "    example['label'] = label2id[example['sentiment']]   # Map string label to integer using the dictionary\n",
    "    return example                                       # Return the updated sample\n",
    "\n",
    "# Apply add_numeric_label function to the entire dataset (train and validation)\n",
    "dataset = dataset.map(add_numeric_label)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Tokenization: entity + text ‚Üí sentence-pair encoding\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "%pip install protobuf\n",
    "\n",
    "# Load the same tokenizer as used by the teacher model to ensure consistency\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.teacher_name, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of examples by treating 'entity' and 'text' as sentence pairs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : Dict[str, List[Any]]\n",
    "        A dictionary containing a batch of samples; keys correspond to data columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[Any]]\n",
    "        A dictionary with tokenizer outputs including input_ids, attention_mask, etc., and the original label.\n",
    "    \"\"\"\n",
    "    # Handle None values by replacing them with empty strings to prevent tokenizer errors\n",
    "    entities = [str(x) if x is not None else '' for x in examples['entity']]\n",
    "    tweets   = [str(x) if x is not None else '' for x in examples['text']]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        entities,                       # Use entity as sentence A\n",
    "        tweets,                         # Use tweet text as sentence B\n",
    "        truncation=True,                # Truncate sequences longer than max_length\n",
    "        padding='max_length',           # Pad all sequences to max_length\n",
    "        max_length=cfg.max_length       # Set the maximum sequence length\n",
    "    )\n",
    "    tokenized['label'] = examples['label']  # Retain the numeric labels\n",
    "    return tokenized\n",
    "\n",
    "# Specify which original columns to remove after tokenization to save memory\n",
    "cols_to_remove = ['id', 'entity', 'sentiment', 'text']\n",
    "\n",
    "# Apply tokenize_function in batched mode and remove unnecessary columns\n",
    "encoded_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove\n",
    ")\n",
    "\n",
    "# Convert the tokenized dataset into PyTorch Tensor format so DataLoader can use it directly\n",
    "encoded_dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b41207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] GPU 34 GB ‚Üí batch_size=32\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5Ô∏è‚É£ DataLoader: Wrap Dataset into Iterable Batches\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Automatically adjust batch_size based on available GPU memory\n",
    "# ---------------------------------------------\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    cfg.batch_size = 64 if total_gb > 40 else 32   # Choose threshold and fallback batch size as needed\n",
    "    print(f\"[Info] GPU {total_gb:.0f} GB ‚Üí batch_size={cfg.batch_size}\")\n",
    "else:\n",
    "    print(\"[Warn] No CUDA device, keep default batch_size\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Create the actual DataLoader using the new cfg.batch_size\n",
    "# ---------------------------------------------\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    encoded_dataset['train'],        # Training set\n",
    "    batch_size=cfg.batch_size,       # Number of samples per batch\n",
    "    shuffle=True                     # Shuffle data at the beginning of each epoch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    encoded_dataset['validation'],   # Validation set\n",
    "    batch_size=cfg.batch_size        # Number of samples per batch\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# General Evaluation Function: Returns (accuracy, macro_F1)\n",
    "# ============================================================\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device: str = \"cuda\"):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        y_true         = batch[\"label\"].to(device)\n",
    "\n",
    "        out = model(input_ids, attention_mask=attention_mask)\n",
    "        # Support both tuple output and object.logits\n",
    "        logits = out[0] if isinstance(out, tuple) else out.logits\n",
    "        y_pred = logits.argmax(dim=1)\n",
    "\n",
    "        preds.append(y_pred.cpu())\n",
    "        labels.append(y_true.cpu())\n",
    "\n",
    "    preds  = torch.cat(preds).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds, average=\"macro\")\n",
    "    return acc, f1\n",
    "\n",
    "print(\"111\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e448e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4Ô∏è‚É£ Model: Configurable Teacher / Student\n",
    "# ============================================================\n",
    "\n",
    "# ---------- Teacher Model (load a pretrained model with a classification head) ----------\n",
    "def build_teacher(cfg, id2label, label2id):\n",
    "    \"\"\"\n",
    "    Build the Teacher model based on cfg.teacher_name.\n",
    "    If cfg.use_lora=True, apply LoRA inside (requires implementation of prepare_lora).\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.teacher_name,            # Model name or path\n",
    "        num_labels = 4,              # Number of target classes for classification\n",
    "        id2label   = id2label,       # Mapping from ID to label string\n",
    "        label2id   = label2id,       # Mapping from label string to ID\n",
    "        trust_remote_code = True     # For compatibility with models like LLaMA / DeBERTa-v3\n",
    "    )\n",
    "\n",
    "    if getattr(cfg, \"use_lora\", False):\n",
    "        model = prepare_lora(model, cfg.lora_cfg)   # You need to implement prepare_lora()\n",
    "    return model.to(cfg.device)                     # Move model to the specified device (GPU/CPU)\n",
    "\n",
    "teacher = build_teacher(cfg, id2label, label2id)   # ‚Üê Replaces hardcoded BERT model loading\n",
    "\n",
    "\n",
    "# ---------- Student Model: Minimal architecture, much smaller than Teacher ----------\n",
    "class TinyStudentMLP(nn.Module):\n",
    "    \"\"\"A very small MLP-based student model (optional)\"\"\"\n",
    "    \"\"\"\n",
    "    Architecture: Embedding ‚Üí Mean Pooling ‚Üí Two Linear Layers ‚Üí Output logits\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)     # Embedding layer, same vocab size as BERT, with 128 dimensions\n",
    "        self.fc1       = nn.Linear(embed_dim, hidden_dim)        # First linear layer: 128 ‚Üí 64\n",
    "        self.fc2       = nn.Linear(hidden_dim, num_labels)       # Output layer: 64 ‚Üí 4 (logits)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : Tensor[B, L]\n",
    "            Sequence of token IDs\n",
    "        attention_mask : Tensor[B, L]\n",
    "            Attention mask (not used; retained for compatibility)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor[B, 4]\n",
    "            Raw (unnormalized) logits\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)  # [B, L, 128]\n",
    "        x = x.mean(dim=1)              # Mean pooling across the sequence dimension ‚Üí [B, 128]\n",
    "        x = F.relu(self.fc1(x))        # Linear layer + ReLU ‚Üí [B, 64]\n",
    "        return self.fc2(x)             # Final logits ‚Üí [B, 4]\n",
    "\n",
    "def build_student(cfg, id2label, label2id, tokenizer):\n",
    "    \"\"\"\n",
    "    Build the student model based on cfg.student_type:\n",
    "      - 'bert-tiny' ‚Üí Load pretrained bert-tiny\n",
    "      - 'tiny-mlp'  ‚Üí Use a custom minimal MLP\n",
    "    \"\"\"\n",
    "    if cfg.student_type == \"bert-tiny\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"prajjwal1/bert-tiny\",\n",
    "            num_labels = 4,\n",
    "            id2label   = id2label,\n",
    "            label2id   = label2id,\n",
    "        )\n",
    "    elif cfg.student_type == \"tiny-mlp\":\n",
    "        model = TinyStudentMLP(vocab_size=tokenizer.vocab_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown student_type: {cfg.student_type}\")\n",
    "    return model.to(cfg.device)\n",
    "\n",
    "student = None   # ‚Üê Leave uninitialized; will be built during the distillation phase\n",
    "\n",
    "\n",
    "# ---------- Unified Distillation Loss (keep a single implementation) ----------\n",
    "#  Combines soft targets and hard targets for knowledge distillation\n",
    "def distillation_loss(student_logits, teacher_logits, labels,\n",
    "                      T: float = 2.0, alpha: float = 0.5):\n",
    "    \"\"\"\n",
    "    Compute hybrid distillation loss:\n",
    "    alpha * cross-entropy (hard labels) + (1 - alpha) * KL divergence (soft labels)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    student_logits : Tensor[B, 4]   Logits from the student model\n",
    "    teacher_logits : Tensor[B, 4]   Logits from the teacher model\n",
    "    labels         : Tensor[B]      Ground-truth hard labels\n",
    "    temperature    : float          Temperature for softening logits\n",
    "    alpha          : float          Weight for the hard label loss\n",
    "    \"\"\"\n",
    "    # ---- Soft target: KL divergence between softened student and teacher probabilities ----\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    )\n",
    "    # ---- Hard target: standard cross-entropy loss to optimize classification accuracy ----\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    # ---- Weighted combination of the two losses ----\n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss * (T * T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d5b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 1/15  loss=1.2739  acc=0.6266  f1=0.5608\n",
      "[Teacher] Epoch 2/15  loss=1.0049  acc=0.7117  f1=0.6927\n",
      "[Teacher] Epoch 3/15  loss=0.8955  acc=0.7518  f1=0.7372\n",
      "[Teacher] Epoch 4/15  loss=0.8039  acc=0.7848  f1=0.7720\n",
      "[Teacher] Epoch 5/15  loss=0.7271  acc=0.8108  f1=0.8033\n",
      "[Teacher] Epoch 6/15  loss=0.6588  acc=0.8248  f1=0.8207\n",
      "[Teacher] Epoch 7/15  loss=0.6115  acc=0.8348  f1=0.8295\n",
      "[Teacher] Epoch 8/15  loss=0.5752  acc=0.8549  f1=0.8486\n",
      "[Teacher] Epoch 9/15  loss=0.5433  acc=0.8599  f1=0.8537\n",
      "[Teacher] Epoch 10/15  loss=0.5195  acc=0.8659  f1=0.8617\n",
      "[Teacher] Epoch 11/15  loss=0.5022  acc=0.8629  f1=0.8578\n",
      "[Teacher] Epoch 12/15  loss=0.4864  acc=0.8649  f1=0.8610\n",
      "‚èπ Ëß¶Âèë Early-Stopping\n",
      "[Teacher] Best valid acc = 0.8659\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6Ô∏è‚É£.5 Final Fine-Tuning of Teacher (Multi-model / LoRA-Optional General Version)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def get_classifier_params(model):\n",
    "    \"\"\"\n",
    "    Extract the classifier head parameters (name varies across architectures).\n",
    "    Returns an empty list if not found.\n",
    "    \"\"\"\n",
    "    for attr in [\"classifier\", \"score\", \"lm_head\"]:\n",
    "        if hasattr(model, attr):\n",
    "            return getattr(model, attr).parameters()\n",
    "    return []\n",
    "\n",
    "def build_layerwise_param_groups(model,\n",
    "                                 base_lr: float = 1e-5,\n",
    "                                 head_lr: float = 5e-5,\n",
    "                                 decay: float   = 0.95):\n",
    "    \"\"\"\n",
    "    Create layer-wise learning rate decay parameter groups for BERT / DeBERTa / LLaMA.\n",
    "    If the model uses PEFT-LoRA, return all trainable parameters with a single learning rate.\n",
    "    \"\"\"\n",
    "    # ‚Äî‚Äî 1) If using LoRA (PEFT), return all trainable parameters ‚Äî‚Äî\n",
    "    if hasattr(model, \"peft_config\"):\n",
    "        return [{\"params\": model.parameters(), \"lr\": head_lr}]\n",
    "\n",
    "    # ‚Äî‚Äî 2) Classifier head parameters ‚Äî‚Äî\n",
    "    param_groups = [{\n",
    "        \"params\": list(get_classifier_params(model)),\n",
    "        \"lr\": head_lr\n",
    "    }]\n",
    "\n",
    "    # ‚Äî‚Äî 3) Encoder parameters with layer-wise decaying learning rates ‚Äî‚Äî\n",
    "    # Define layer regex patterns by architecture\n",
    "    layer_patterns = {\n",
    "        \"bert\"   : r\"encoder\\.layer\\.(\\d+)\\.\",\n",
    "        \"deberta\": r\"encoder\\.layer\\.(\\d+)\\.\",\n",
    "        \"llama\"  : r\"model\\.layers\\.(\\d+)\\.\",\n",
    "    }\n",
    "    # Infer architecture type\n",
    "    name_keys = list(dict(model.named_parameters()).keys())\n",
    "    arch = \"llama\" if any(k.startswith(\"model.layers.\") for k in name_keys) \\\n",
    "           else (\"deberta\" if \"deberta\" in model.__class__.__name__.lower() else \"bert\")\n",
    "    pat = re.compile(layer_patterns[arch])\n",
    "\n",
    "    # Count total number of layers\n",
    "    n_layers = 0\n",
    "    for n in name_keys:\n",
    "        m = pat.search(n)\n",
    "        if m:\n",
    "            n_layers = max(n_layers, int(m.group(1)) + 1)\n",
    "\n",
    "    # Group parameters by layer index with decayed learning rate\n",
    "    params_dict = dict(model.named_parameters())\n",
    "    for idx in range(n_layers):\n",
    "        lr_i = base_lr * (decay ** (n_layers - 1 - idx))  # Later layers get higher lr\n",
    "        layer_params = [\n",
    "            p for n, p in params_dict.items()\n",
    "            if (m := pat.search(n)) and int(m.group(1)) == idx\n",
    "        ]\n",
    "        if layer_params:  # Some layers may have no parameters\n",
    "            param_groups.append({\"params\": layer_params, \"lr\": lr_i})\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "# ---------- Automatically Set Epochs and Patience Based on Model Size ----------\n",
    "if not hasattr(cfg, \"teacher_epochs\"):  # Use cfg values if already set\n",
    "    if \"large\" in cfg.teacher_name.lower():\n",
    "        cfg.teacher_epochs, cfg.teacher_patience = 9, 1\n",
    "    elif \"llama\" in cfg.teacher_name.lower():\n",
    "        cfg.teacher_epochs, cfg.teacher_patience = 6, 1\n",
    "    else:  # base / tiny models\n",
    "        cfg.teacher_epochs, cfg.teacher_patience = 15, 2\n",
    "\n",
    "# ---------- Build Optimizer & Scheduler ----------\n",
    "param_groups = build_layerwise_param_groups(\n",
    "    teacher,\n",
    "    base_lr = 1e-5,\n",
    "    head_lr = 5e-5,\n",
    "    decay   = 0.95\n",
    ")\n",
    "optimizer_t = AdamW(param_groups, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * cfg.teacher_epochs\n",
    "scheduler_t = get_linear_schedule_with_warmup(\n",
    "    optimizer_t,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "best_acc, patience_ctr, best_state = 0.0, 0, None\n",
    "\n",
    "t0 = time.time()  # Start timing Teacher training\n",
    "\n",
    "for epoch in range(cfg.teacher_epochs):\n",
    "    teacher.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids      = batch[\"input_ids\"].to(cfg.device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(cfg.device, non_blocking=True)\n",
    "        labels         = batch[\"label\"].to(cfg.device, non_blocking=True)\n",
    "\n",
    "        logits = teacher(input_ids, attention_mask=attention_mask).logits\n",
    "        loss   = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer_t.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(teacher.parameters(), 1.0)\n",
    "        optimizer_t.step()\n",
    "        scheduler_t.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ‚Äî‚Äî Evaluation ‚Äî‚Äî (assumes `evaluate()` returns accuracy and F1 score)\n",
    "    val_acc, val_f1 = evaluate(teacher, valid_loader, cfg.device)\n",
    "    print(f\"[Teacher] Epoch {epoch+1}/{cfg.teacher_epochs}  \"\n",
    "          f\"loss={running_loss/len(train_loader):.4f}  \"\n",
    "          f\"acc={val_acc:.4f}  f1={val_f1:.4f}\")\n",
    "\n",
    "    # ‚Äî‚Äî Early Stopping Logic ‚Äî‚Äî\n",
    "    if val_acc > best_acc + 1e-3:\n",
    "        best_acc = val_acc\n",
    "        best_state = {k: v.clone() for k, v in teacher.state_dict().items()}\n",
    "        patience_ctr = 0\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= cfg.teacher_patience:\n",
    "            print(\"‚èπ Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "t1 = time.time()  # End timing Teacher training\n",
    "\n",
    "# ---------- Save Best Model ----------\n",
    "teacher.load_state_dict(best_state)\n",
    "teacher.eval()\n",
    "model_tag = cfg.teacher_name.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "torch.save(teacher.state_dict(), f\"teacher_best_{model_tag}.pt\")\n",
    "print(f\"[Teacher] Best validation accuracy = {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee272323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Teacher acc=0.8659  f1=0.8617\n",
      "\n",
      "=== ËØÑ‰º∞ÊåáÊ†áÊ±áÊÄª ===\n",
      "Teacher ËÆ≠ÁªÉÊó∂Èó¥:   1338.2s    Â≥∞ÂÄºÊòæÂ≠ò: 2.8GB\n",
      "Teacher Êé®ÁêÜÈÄüÂ∫¶:   456.2 Ê†∑Êú¨/s\n",
      "Teacher ÂèÇÊï∞ËßÑÊ®°:   184,425,220 ‰∏™ÂèÇÊï∞\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8Ô∏è‚É£ Unified Evaluation & Saving\n",
    "# ============================================================\n",
    "\n",
    "# ---------------- Accuracy & F1 --------------------\n",
    "\n",
    "model_tag = cfg.teacher_name.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "\n",
    "# Optionally print Teacher vs. Student metrics\n",
    "t_acc, t_f1 = evaluate(teacher, valid_loader, cfg.device)\n",
    "print(f\"| Teacher acc={t_acc:.4f}  f1={t_f1:.4f}\")\n",
    "\n",
    "\n",
    "# ------------ Final evaluation for both teacher and student models -----------------------------\n",
    "# ------ Including: training time, peak memory usage, inference speed (samples/sec), and parameter count ------\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def measure_inference_speed(model, dataloader, device, max_batches=None):\n",
    "    \"\"\"Run through the entire dataloader (or first `max_batches` batches) and return throughput in samples/sec\"\"\"\n",
    "    model.eval()\n",
    "    torch.cuda.synchronize(device)\n",
    "    start = time.time()\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "            n_samples += input_ids.size(0)\n",
    "            if max_batches and i + 1 >= max_batches:\n",
    "                break\n",
    "    torch.cuda.synchronize(device)\n",
    "    elapsed = time.time() - start\n",
    "    return n_samples / elapsed\n",
    "\n",
    "# ‚Äî‚Äî Reset peak memory statistics before measuring ‚Äî‚Äî\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "# ‚Äî‚Äî 1) Teacher training time & memory ‚Äî‚Äî \n",
    "\n",
    "teacher_train_time = t1 - t0                              # in seconds\n",
    "teacher_peak_mem  = torch.cuda.max_memory_allocated(device) / 1024**3  # in GB\n",
    "\n",
    "\n",
    "\n",
    "# ‚Äî‚Äî 3) Inference Speed ‚Äî‚Äî \n",
    "\n",
    "teacher_throughput = measure_inference_speed(\n",
    "    teacher,\n",
    "    valid_loader,\n",
    "    device\n",
    ")\n",
    "\n",
    "\n",
    "# ‚Äî‚Äî 4) Parameter Count ‚Äî‚Äî \n",
    "teacher_params = count_parameters(teacher)\n",
    "# student_params = count_parameters(final_student)\n",
    "\n",
    "# ‚Äî‚Äî Print Evaluation Summary ‚Äî‚Äî \n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Teacher Training Time:   {teacher_train_time:.1f}s    Peak Memory: {teacher_peak_mem:.1f} GB\")\n",
    "# print(f\"Student Training Time:   {student_train_time:.1f}s    Peak Memory: {student_peak_mem:.1f} GB\")\n",
    "print(f\"Teacher Inference Speed: {teacher_throughput:.1f} samples/sec\")\n",
    "# print(f\"Student Inference Speed: {student_throughput:.1f} samples/sec\")\n",
    "print(f\"Teacher Parameter Count: {teacher_params:,} parameters\")\n",
    "# print(f\"Student Parameter Count: {student_params:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e502094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Êé®ÁêÜÂêûÂêê: 475.9 samples/s, Â≥∞ÂÄºÊòæÂ≠ò: 3.2 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ====================================================\n",
    "# ‚Äî‚Äî Evaluation: Inference Speed & Peak Memory Usage ‚Äî‚Äî\n",
    "# ====================================================\n",
    "import torch\n",
    "\n",
    "# ‚Äî‚Äî A) Evaluate Teacher ‚Äî‚Äî\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "teacher_throughput = measure_inference_speed(\n",
    "    teacher, valid_loader, device\n",
    ")\n",
    "teacher_peak_mem = torch.cuda.max_memory_allocated(device) / 1024**3\n",
    "print(f\"Teacher Inference Throughput: {teacher_throughput:.1f} samples/s, Peak Memory: {teacher_peak_mem:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ed74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16fbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30c760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac5dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839b9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efa631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c932bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17138d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
